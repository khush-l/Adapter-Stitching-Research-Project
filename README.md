# Adapter Stitching for Efficient Fine-Tuning

## Abstract
The rapid advancement of large language models (LLMs) has enabled remarkable progress in natural language processing (NLP) tasks. However, the sheer size of these models, often consisting of hundreds of billions of parameters, imposes significant computational costs. Traditional fine-tuning approaches, which involve updating all model parameters for new tasks, are increasingly impractical due to the associated resource demands. To address these challenges, parameter-efficient fine-tuning (PEFT) methods, like adapters, have been developed to reduce the computational footprint by updating only a small subset of parameters. While effective, these approaches still require tuning additional parameters for each new task, leading to increased storage requirements when scaling to multiple tasks. In this project, we propose a novel method called Adapter Stitching, which extends the concept of model stitching to fine-tune GPT-2 efficiently across multiple tasks. Instead of fully fine-tuning or using conventional PEFT methods, we stitch together pre-trained adapters from different tasks, allowing the model backbone to remain entirely frozen. We evaluated Adapter Stitching in comparison to traditional full-model fine-tuning and existing PEFT methods, assessing its performance, computational efficiency, and scalability across various NLP tasks.

## Introduction
In recent years, the widespread use of large language models (LLMs) such as [GPT-3](https://arxiv.org/abs/2005.14165), [GPT-4](https://arxiv.org/abs/2303.08774), [LLama 3](https://arxiv.org/abs/2407.21783), and [Mistral 7B](https://arxiv.org/abs/2310.06825) has led to significant advancements in natural language processing (NLP) tasks such as language understanding, text generation, and complex reasoning [(1)](https://arxiv.org/abs/2302.06476). However, the large size of these models, often with hundreds of billions of parameters, creates substantial challenges in terms of computational cost [(2)](https://proceedings.mlr.press/v162/du22c.html). Particularly, when fine-tuning models for specific downstream tasks, these costs remain high [(3)](https://arxiv.org/abs/1910.10683). Traditional full-model fine-tuning (FFT) involves updating all parameters of a pre-trained model to adapt it to a new task, which is not only computationally expensive but also inefficient when considering the resource requirements for both training and inference [(4)](https://arxiv.org/abs/2005.14165). As these LLMs continue to grow in size, full model fine-tuning becomes increasingly impractical due to the need to store multiple fine-tuned versions of these models.

To address these issues, a range of parameter-efficient fine-tuning (PEFT) techniques have been proposed, allowing only a small subset of the model’s parameters to be updated during the fine-tuning process [(5)](http://arxiv.org/abs/1902.00751). PEFT adapts an LLM for downstream tasks by keeping the entire LLM backbone frozen and updating only a small set of newly introduced parameters [(6)](https://arxiv.org/pdf/2312.03863). PEFT techniques have gained popularity due to their ability to reduce computational cost and memory footprint while achieving near-comparable performance to full fine-tuning [(7)](https://arxiv.org/abs/2405.15525v1).

[Adapters](https://arxiv.org/abs/1902.00751), first introduced by Houlsby et al. (2019), are one of the most efficient PEFT methods. Adapters consist of lightweight, bottleneck-shaped modules that are inserted between the layers of a neural network, allowing the rest of the model to remain frozen during fine-tuning. This dramatically reduces the number of trainable parameters to around 0.1%–0.5% of the total model, making the fine-tuning process much more efficient [(9)](https://arxiv.org/abs/2005.00052). More recently, [AdaMix](https://arxiv.org/abs/2205.12410) (Wang et al., 2022) improved this idea by integrating a mixture of adaptation modules, each responsible for different aspects of the fine-tuning process, leading to better performance across a range of NLP tasks.

Although adapters are effective, they rely on updating additional parameters during fine-tuning, which can lead to increased costs when applied to multiple tasks. As the number of downstream tasks increases, so does the number of fine-tuned parameters and models, which complicates deployment and results in higher storage costs.

Model stitching is a way to leverage pre-trained components across different tasks without needing to fine-tune the entire model for each task [(10)](https://arxiv.org/abs/1411.5908). Model stitching involves "stitching" together parts of different pre-trained networks to create a new model that is adaptable to specific tasks, without the need to fully re-train the entire model. Model stitching has also been used in many other fields such as neuroevolution, LLM debiasing, and GAN visualization. [Network Stitching](https://arxiv.org/abs/2106.07682), introduced by Bansal et al. (2021), showed that stitching layers of pre-trained networks can be a more efficient alternative to training a new model from scratch or performing full fine-tuning. The flexibility of stitching allows you to dynamically reuse model components across multiple tasks, which can drastically reduce storage and computational costs.

In this paper, we extend the concept of model stitching to the fine-tuning of large language models by integrating model stitching with adapter-based tuning. Instead of fully fine-tuning or using PEFT methods that still require tuning small modules, we propose a novel approach that stitches pre-trained adapters from different tasks, while keeping the rest of the model frozen. Our approach is different from existing work on model stitching by focusing on adapters rather than general neural network components or other PEFT methods. We propose a more resource-efficient approach to LLM fine-tuning by freezing the majority of the model's layers and stitching together adapters from different tasks. This method preserves task-specific performance while minimizing both storage and computational costs.


